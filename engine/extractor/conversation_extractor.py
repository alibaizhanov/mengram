"""
Conversation Extractor v2 ‚Äî –∏–∑–≤–ª–µ–∫–∞–µ—Ç RICH –∑–Ω–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤.

–ò–∑–≤–ª–µ–∫–∞–µ—Ç:
1. Entities (person, project, technology, company, concept)
2. Facts ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
3. Relations ‚Äî —Å–≤—è–∑–∏ –º–µ–∂–¥—É entities
4. Knowledge ‚Äî —Ä–µ—à–µ–Ω–∏—è, —Ñ–æ—Ä–º—É–ª—ã, —Ä–µ—Ü–µ–ø—Ç—ã, –∫–æ–Ω—Ñ–∏–≥–∏, –∫–æ–º–∞–Ω–¥—ã (—Å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏)

Knowledge ‚Äî —ç—Ç–æ killer feature. LLM —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–Ω–∞–Ω–∏—è:
  [solution] ‚Äî —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (–∫–æ–¥, –∫–æ–Ω—Ñ–∏–≥)
  [formula] ‚Äî —Ñ–æ—Ä–º—É–ª–∞, —É—Ä–∞–≤–Ω–µ–Ω–∏–µ
  [treatment] ‚Äî –ª–µ—á–µ–Ω–∏–µ, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ
  [experiment] ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
  [recipe] ‚Äî —Ä–µ—Ü–µ–ø—Ç (–∫—É–ª–∏–Ω–∞—Ä–∏—è, –ø—Ä–æ—Ü–µ—Å—Å)
  [decision] ‚Äî –ø—Ä–∏–Ω—è—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ
  [command] ‚Äî –ø–æ–ª–µ–∑–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ / –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
  [reference] ‚Äî —Å—Å—ã–ª–∫–∞, –∏—Å—Ç–æ—á–Ω–∏–∫
  [insight] ‚Äî –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –∏–Ω—Å–∞–π—Ç
  [example] ‚Äî –ø—Ä–∏–º–µ—Ä, –∫–µ–π—Å
  ... –ª—é–±–æ–π –¥—Ä—É–≥–æ–π —Ç–∏–ø –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ —Å–º—ã—Å–ª—É
"""

import sys
import json
from dataclasses import dataclass, field
from typing import Optional

from engine.extractor.llm_client import LLMClient


EXTRACTION_PROMPT = """You are a precision knowledge extraction system. Extract IMPORTANT, LASTING personal knowledge from the USER's messages.

Return ONLY valid JSON without markdown. Be VERY selective ‚Äî quality over quantity.

WHO TO EXTRACT ABOUT:
- Extract knowledge revealed by the USER about themselves, their projects, their life
- DO NOT extract general knowledge the assistant explained (e.g. "nginx config is at /etc/nginx" ‚Äî unless user confirms they used it)
- If assistant suggests something and user says "yes, I did that" or "that worked" ‚Äî extract the user's action/result
- Focus on: user's identity, skills, preferences, projects, decisions, workflows, relationships

ENTITY RULES:
- ONLY named, specific entities with 2+ extractable facts
- If something is mentioned once in passing ‚Äî make it a fact on the parent entity, NOT a separate entity
  GOOD: Entity "Mengram" ‚Üí fact: "uses Redis as cache"
  BAD: Entity "Redis" ‚Üí fact: "used as cache" (only 1 fact, make it a fact on the project instead)
- entity_type: person, project, technology, company, concept
- If user says "I"/"me"/"my" ‚Äî resolve to their name if known, otherwise "User"
{existing_context}
ENTITY NAMING:
- EXACT casing from context: "Mengram" not "MENGRAM", "PostgreSQL" not "postgresql"
- FULL official name: "Ali Baizhanov" not "Ali", "Uzum Bank" not "uzum"
- If entity already exists above ‚Äî use EXACT SAME NAME (do not create duplicates)

FACT RULES:
- Normalized format: subject + verb + object, present tense, under 10 words
  GOOD: "uses Python", "deployed on Railway", "prefers dark mode"
  BAD: "he has been using Python for a while now", "is currently in the process of deploying"
- ONLY facts that DIRECTLY describe the entity they're assigned to
- Keep project facts on projects, personal facts on the person ‚Äî don't mix
- DO NOT extract: temporary actions ("asked about X"), session events ("sent a message"), assistant's explanations

FACT DEDUP ‚Äî check existing facts above. Do NOT re-extract facts that already exist (even if worded slightly differently).
If user says "I use Python" and existing context already has "uses Python" ‚Üí skip it.
If user says "I switched from React to Svelte" and existing has "uses React" ‚Üí extract "switched to Svelte" (this is NEW info).

EPISODIC MEMORY ‚Äî extract noteworthy events/interactions:
- An episode = something that HAPPENED: a discussion, decision, debugging session, milestone, problem solved
- Only extract if the event is meaningful and worth remembering (not trivial chat)
- Include: what happened (summary), details (context), result (outcome), who/what was involved (participants)
- emotional_valence: positive (success, achievement), negative (failure, frustration), neutral, mixed
- importance: 0.3 (minor event) to 0.9 (major decision/milestone)
- Do NOT create episodes for routine exchanges with no meaningful outcome

PROCEDURAL MEMORY ‚Äî extract learned workflows/processes:
- A procedure = a repeatable sequence of steps the user performs or described
- Only extract if there are 2+ concrete steps forming a workflow
- Include: name (what the procedure does), trigger (when to use it), steps (ordered actions)
- Link to entities involved
- Do NOT create procedures from hypothetical/planned workflows ‚Äî only from confirmed actions

Response format (strict JSON, no ```):
{{
  "entities": [
    {{
      "name": "Entity Name",
      "type": "person|project|technology|company|concept",
      "facts": ["fact 1", "fact 2"]
    }}
  ],
  "relations": [
    {{
      "from": "Entity 1",
      "to": "Entity 2",
      "type": "works_at|uses|member_of|related_to|depends_on|created_by",
      "description": "short description"
    }}
  ],
  "knowledge": [
    {{
      "entity": "Entity this knowledge belongs to",
      "type": "solution|formula|command|insight|decision|recipe|reference",
      "title": "Short descriptive title",
      "content": "Detailed explanation",
      "artifact": "code/config/formula/command (optional, null if none)"
    }}
  ],
  "episodes": [
    {{
      "summary": "Brief description of what happened (under 15 words)",
      "context": "Detailed description of the event and discussion",
      "outcome": "What was decided, resolved, or resulted",
      "participants": ["Entity1", "Entity2"],
      "emotional_valence": "positive|negative|neutral|mixed",
      "importance": 0.5
    }}
  ],
  "procedures": [
    {{
      "name": "Short procedure name",
      "trigger": "When/why to use this procedure",
      "steps": [
        {{"step": 1, "action": "What to do", "detail": "Specific command or instruction"}},
        {{"step": 2, "action": "Next step", "detail": "Specifics"}}
      ],
      "entities": ["Entity1"]
    }}
  ]
}}

EXAMPLE:
Input conversation:
  User: "–Ø –≤—á–µ—Ä–∞ –∑–∞–¥–µ–ø–ª–æ–∏–ª mengram –Ω–∞ Railway, –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç. –ü—Ä–∏—à–ª–æ—Å—å –ø–æ–≤–æ–∑–∏—Ç—å—Å—è —Å pgvector"
  Assistant: "–û—Ç–ª–∏—á–Ω–æ! –ö–∞–∫–∞—è –≤–µ—Ä—Å–∏—è PostgreSQL?"
  User: "15-—è, –Ω–∞ Supabase —Ö–æ—Å—Ç–∏—Ç—Å—è. –ü—Ä–æ—Ü–µ—Å—Å —Ç–∞–∫–æ–π: —Å–Ω–∞—á–∞–ª–∞ build, –ø–æ—Ç–æ–º twine upload, –ø–æ—Ç–æ–º npm publish"

Output:
{{
  "entities": [
    {{"name": "Mengram", "type": "project", "facts": ["deployed on Railway", "uses pgvector extension"]}},
    {{"name": "Supabase", "type": "technology", "facts": ["hosts PostgreSQL 15 for Mengram"]}}
  ],
  "relations": [
    {{"from": "Mengram", "to": "Railway", "type": "depends_on", "description": "deployed on"}},
    {{"from": "Mengram", "to": "Supabase", "type": "depends_on", "description": "database hosting"}}
  ],
  "knowledge": [],
  "episodes": [
    {{
      "summary": "Successfully deployed Mengram on Railway",
      "context": "Deployed Mengram to Railway. Had issues with pgvector extension that required debugging. Uses Supabase with PostgreSQL 15 as the database.",
      "outcome": "Deployment successful, everything working",
      "participants": ["Mengram", "Railway", "Supabase"],
      "emotional_valence": "positive",
      "importance": 0.7
    }}
  ],
  "procedures": [
    {{
      "name": "Release Mengram package",
      "trigger": "When publishing a new version of Mengram",
      "steps": [
        {{"step": 1, "action": "Build Python package", "detail": "python -m build"}},
        {{"step": 2, "action": "Upload to PyPI", "detail": "twine upload dist/*"}},
        {{"step": 3, "action": "Publish npm package", "detail": "npm publish"}}
      ],
      "entities": ["Mengram"]
    }}
  ]
}}

CONVERSATION:
{conversation}

Extract knowledge (return ONLY JSON):"""


EXISTING_CONTEXT_BLOCK = """
EXISTING ENTITIES FOR THIS USER (use same names, avoid duplicate facts):
{context}
"""


@dataclass
class ExtractedEntity:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å"""
    name: str
    entity_type: str  # person, project, technology, company, concept
    facts: list[str] = field(default_factory=list)

    def __repr__(self):
        return f"Entity({self.entity_type}: {self.name}, facts={len(self.facts)})"


@dataclass
class ExtractedRelation:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω–∞—è —Å–≤—è–∑—å"""
    from_entity: str
    to_entity: str
    relation_type: str
    description: str = ""

    def __repr__(self):
        return f"Relation({self.from_entity} --{self.relation_type}--> {self.to_entity})"


@dataclass
class ExtractedKnowledge:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ ‚Äî solution, formula, command, etc."""
    entity: str           # –∫ –∫–∞–∫–æ–π entity –æ—Ç–Ω–æ—Å–∏—Ç—Å—è
    knowledge_type: str   # solution, formula, treatment, command, insight, ...
    title: str            # –∫—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    content: str          # –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    artifact: Optional[str] = None  # –∫–æ–¥, –∫–æ–Ω—Ñ–∏–≥, —Ñ–æ—Ä–º—É–ª–∞, –∫–æ–º–∞–Ω–¥–∞

    def __repr__(self):
        has_artifact = "üìé" if self.artifact else ""
        return f"Knowledge([{self.knowledge_type}] {self.title} ‚Üí {self.entity} {has_artifact})"


@dataclass
class ExtractedEpisode:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —ç–ø–∏–∑–æ–¥ ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ."""
    summary: str                  # –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–¥–æ 15 —Å–ª–æ–≤)
    context: str = ""             # –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    outcome: str = ""             # —Ä–µ–∑—É–ª—å—Ç–∞—Ç/—Ä–µ—à–µ–Ω–∏–µ
    participants: list[str] = field(default_factory=list)  # —É—á–∞—Å—Ç–≤—É—é—â–∏–µ entities
    emotional_valence: str = "neutral"  # positive/negative/neutral/mixed
    importance: float = 0.5       # 0.0-1.0

    def __repr__(self):
        return f"Episode({self.summary[:50]}... [{self.emotional_valence}])"


@dataclass
class ExtractedProcedure:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ ‚Äî –ø–æ–≤—Ç–æ—Ä—è–µ–º—ã–π workflow/–Ω–∞–≤—ã–∫."""
    name: str                     # –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
    trigger: str = ""             # –∫–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å
    steps: list[dict] = field(default_factory=list)  # [{step, action, detail}]
    entities: list[str] = field(default_factory=list)  # —Å–≤—è–∑–∞–Ω–Ω—ã–µ entities

    def __repr__(self):
        return f"Procedure({self.name}, steps={len(self.steps)})"


@dataclass
class ExtractionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
    entities: list[ExtractedEntity] = field(default_factory=list)
    relations: list[ExtractedRelation] = field(default_factory=list)
    knowledge: list[ExtractedKnowledge] = field(default_factory=list)
    episodes: list[ExtractedEpisode] = field(default_factory=list)
    procedures: list[ExtractedProcedure] = field(default_factory=list)
    raw_response: str = ""

    def __repr__(self):
        return (
            f"ExtractionResult(entities={len(self.entities)}, "
            f"relations={len(self.relations)}, "
            f"knowledge={len(self.knowledge)}, "
            f"episodes={len(self.episodes)}, "
            f"procedures={len(self.procedures)})"
        )


class ConversationExtractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤"""

    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def extract(self, conversation: list[dict], existing_context: str = "") -> ExtractionResult:
        conv_text = self._format_conversation(conversation)

        # Build context block
        if existing_context:
            context_block = EXISTING_CONTEXT_BLOCK.format(context=existing_context)
        else:
            context_block = ""

        prompt = EXTRACTION_PROMPT.format(
            conversation=conv_text,
            existing_context=context_block
        )
        raw_response = self.llm.complete(prompt)
        return self._parse_response(raw_response)

    def extract_from_text(self, text: str) -> ExtractionResult:
        conversation = [{"role": "user", "content": text}]
        return self.extract(conversation)

    def _format_conversation(self, conversation: list[dict]) -> str:
        lines = []
        for msg in conversation:
            role = "User" if msg["role"] == "user" else "Assistant"
            lines.append(f"{role}: {msg['content']}")
        return "\n\n".join(lines)

    def _parse_response(self, raw: str) -> ExtractionResult:
        result = ExtractionResult(raw_response=raw)

        # Clean markdown
        clean = raw.strip()
        if clean.startswith("```"):
            lines = clean.split("\n")
            clean = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])

        try:
            data = json.loads(clean)
        except json.JSONDecodeError:
            start = raw.find("{")
            end = raw.rfind("}") + 1
            if start >= 0 and end > start:
                try:
                    data = json.loads(raw[start:end])
                except json.JSONDecodeError:
                    print(f"‚ö†Ô∏è  Failed to parse JSON from LLM", file=sys.stderr)
                    return result
            else:
                print(f"‚ö†Ô∏è  LLM returned no JSON", file=sys.stderr)
                return result

        # Entities
        for e in data.get("entities", []):
            result.entities.append(ExtractedEntity(
                name=e.get("name", "Unknown"),
                entity_type=e.get("type", "concept"),
                facts=e.get("facts", []),
            ))

        # Relations
        for r in data.get("relations", []):
            result.relations.append(ExtractedRelation(
                from_entity=r.get("from", ""),
                to_entity=r.get("to", ""),
                relation_type=r.get("type", "related_to"),
                description=r.get("description", ""),
            ))

        # Knowledge (NEW)
        for k in data.get("knowledge", []):
            result.knowledge.append(ExtractedKnowledge(
                entity=k.get("entity", ""),
                knowledge_type=k.get("type", "insight"),
                title=k.get("title", ""),
                content=k.get("content", ""),
                artifact=k.get("artifact"),
            ))

        # Episodes (v2.5)
        for ep in data.get("episodes", []):
            result.episodes.append(ExtractedEpisode(
                summary=ep.get("summary", ""),
                context=ep.get("context", ""),
                outcome=ep.get("outcome", ""),
                participants=ep.get("participants", []),
                emotional_valence=ep.get("emotional_valence", "neutral"),
                importance=float(ep.get("importance", 0.5)),
            ))

        # Procedures (v2.5)
        for pr in data.get("procedures", []):
            result.procedures.append(ExtractedProcedure(
                name=pr.get("name", ""),
                trigger=pr.get("trigger", ""),
                steps=pr.get("steps", []),
                entities=pr.get("entities", []),
            ))

        return result


# --- Mock for testing ---

class MockLLMClient(LLMClient):
    """Mock LLM for testing without API"""

    def complete(self, prompt: str, system: str = "") -> str:
        return json.dumps({
            "entities": [
                {
                    "name": "User",
                    "type": "person",
                    "facts": [
                        "–†–∞–±–æ—Ç–∞–µ—Ç backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º",
                        "–†–∞–±–æ—Ç–∞–µ—Ç –≤ Uzum Bank",
                        "–û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–µ–∫: Java, Spring Boot"
                    ]
                },
                {
                    "name": "Uzum Bank",
                    "type": "company",
                    "facts": ["–ë–∞–Ω–∫ –≤ –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ", "–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"]
                },
                {
                    "name": "–ü—Ä–æ–µ–∫—Ç Alpha",
                    "type": "project",
                    "facts": ["Backend —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–ª–∞—Ç–µ–∂–µ–π", "–ü—Ä–æ–±–ª–µ–º–∞ —Å connection pool"]
                },
                {
                    "name": "PostgreSQL",
                    "type": "technology",
                    "facts": ["–û—Å–Ω–æ–≤–Ω–∞—è –ë–î", "–í–µ—Ä—Å–∏—è 15"]
                },
                {
                    "name": "Spring Boot",
                    "type": "technology",
                    "facts": ["–û—Å–Ω–æ–≤–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤"]
                }
            ],
            "relations": [
                {"from": "User", "to": "Uzum Bank", "type": "works_at", "description": "Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"},
                {"from": "User", "to": "–ü—Ä–æ–µ–∫—Ç Alpha", "type": "member_of", "description": "–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º"},
                {"from": "–ü—Ä–æ–µ–∫—Ç Alpha", "to": "PostgreSQL", "type": "uses", "description": "–û—Å–Ω–æ–≤–Ω–∞—è –ë–î"},
                {"from": "–ü—Ä–æ–µ–∫—Ç Alpha", "to": "Spring Boot", "type": "uses", "description": "Backend —Ñ—Ä–µ–π–º–≤–æ—Ä–∫"},
                {"from": "Uzum Bank", "to": "–ü—Ä–æ–µ–∫—Ç Alpha", "type": "related_to", "description": "–ü—Ä–æ–µ–∫—Ç –±–∞–Ω–∫–∞"}
            ],
            "knowledge": [
                {
                    "entity": "PostgreSQL",
                    "type": "solution",
                    "title": "Connection pool exhaustion fix",
                    "content": "OOM –ø—Ä–∏ 200+ WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è—Ö. –ö–∞–∂–¥—ã–π WS –¥–µ—Ä–∂–∞–ª –æ—Ç–¥–µ–ª—å–Ω—ã–π connection. –†–µ—à–µ–Ω–∏–µ: Redis –∫–µ—à –¥–ª—è UserService –∏ BlockedAccountService.",
                    "artifact": "spring.datasource.hikari.maximum-pool-size: 20\nspring.datasource.hikari.idle-timeout: 30000\nspring.datasource.hikari.connection-timeout: 5000"
                },
                {
                    "entity": "PostgreSQL",
                    "type": "command",
                    "title": "Check active connections",
                    "content": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π PostgreSQL",
                    "artifact": "SELECT count(*), state FROM pg_stat_activity GROUP BY state;"
                }
            ],
            "episodes": [
                {
                    "summary": "Debugged PostgreSQL connection pool exhaustion",
                    "context": "200+ WebSocket connections caused OOM. Each WS held a separate DB connection. Investigated HikariCP settings.",
                    "outcome": "Fixed by adding Redis cache for UserService and BlockedAccountService, reduced pool size to 20",
                    "participants": ["PostgreSQL", "–ü—Ä–æ–µ–∫—Ç Alpha"],
                    "emotional_valence": "positive",
                    "importance": 0.7
                }
            ],
            "procedures": [
                {
                    "name": "Debug PostgreSQL connection issues",
                    "trigger": "When database connections are exhausted or OOM occurs",
                    "steps": [
                        {"step": 1, "action": "Check active connections", "detail": "SELECT count(*), state FROM pg_stat_activity GROUP BY state;"},
                        {"step": 2, "action": "Review HikariCP pool settings", "detail": "Check maximum-pool-size, idle-timeout, connection-timeout"},
                        {"step": 3, "action": "Add caching layer", "detail": "Use Redis to cache frequently accessed services"}
                    ],
                    "entities": ["PostgreSQL", "–ü—Ä–æ–µ–∫—Ç Alpha"]
                }
            ]
        }, ensure_ascii=False)
